{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ww8LEDrUff3p",
        "outputId": "4eebb0e7-4de7-47fc-a5a1-536971954289"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-17 13:43:41--  https://raw.githubusercontent.com/cbannard/lela60331_24-25/refs/heads/main/coursework/Compiled_Reviews.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22322605 (21M) [text/plain]\n",
            "Saving to: ‘Compiled_Reviews.txt’\n",
            "\n",
            "Compiled_Reviews.tx 100%[===================>]  21.29M  80.0MB/s    in 0.3s    \n",
            "\n",
            "2025-01-17 13:43:41 (80.0 MB/s) - ‘Compiled_Reviews.txt’ saved [22322605/22322605]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/cbannard/lela60331_24-25/refs/heads/main/coursework/Compiled_Reviews.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ],
      "metadata": {
        "id": "gFVAQPDefpHC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews=[]\n",
        "sentiment_ratings=[]\n",
        "product_types=[]\n",
        "helpfulness_ratings=[]\n",
        "\n",
        "with open(\"Compiled_Reviews.txt\") as f:\n",
        "   for line in f.readlines()[1:]:\n",
        "        fields = line.rstrip().split('\\t')\n",
        "        reviews.append(fields[0])\n",
        "        sentiment_ratings.append(fields[1])\n",
        "        product_types.append(fields[2])\n",
        "        helpfulness_ratings.append(fields[3])"
      ],
      "metadata": {
        "id": "0lwWBDDkfsB8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_def = re.compile(r\"[\\w]+|[^\\w\\s]\")\n",
        "tokenized_sents = [token_def.findall(txt) for txt in reviews]\n",
        "tokens=[]\n",
        "for s in tokenized_sents:\n",
        "      tokens.extend(s)\n",
        "counts=Counter(tokens)\n",
        "so=sorted(counts.items(), key=lambda item: item[1], reverse=True)\n",
        "so=list(zip(*so))[0]\n",
        "type_list=so[0:10000]"
      ],
      "metadata": {
        "id": "5Nn1sJQifuY0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a 36547 x 10000 matrix of zeros\n",
        "M = np.zeros((len(reviews), len(type_list)))\n",
        "#iterate over the reviews\n",
        "for i, rev in enumerate(reviews):\n",
        "    # Tokenise the current review:\n",
        "    tokens = set(token_def.findall(rev))\n",
        "    # iterate over the words in our type list (the set of 5000 words):\n",
        "    for j,t in enumerate(type_list):\n",
        "        # if the current word j occurs in the current review i then set the matrix element at i,j to be one. Otherwise leave as zero.\n",
        "        if t in tokens:\n",
        "              M[i,j] = 1"
      ],
      "metadata": {
        "id": "n-Lxi-spf2VG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ints=np.random.choice(len(reviews),int(len(reviews)*0.8),replace=False)\n",
        "test_ints=list(set(range(0,len(reviews))) - set(train_ints))\n",
        "M_train = M[train_ints,]\n",
        "M_test = M[test_ints,]\n",
        "labels = sentiment_ratings\n",
        "labels_train = [labels[i] for i in train_ints]\n",
        "labels_test = [labels[i] for i in test_ints]"
      ],
      "metadata": {
        "id": "WzpzbXUhf3BR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_features=10000\n",
        "y=[int(l == \"positive\") for l in labels_train]\n",
        "weights = np.random.rand(num_features)\n",
        "bias=np.random.rand(1)\n",
        "n_iters = 7000\n",
        "lr=0.07\n",
        "logistic_loss=[]\n",
        "num_samples=len(y)\n",
        "for i in range(n_iters):\n",
        "  z = M_train.dot(weights)+bias\n",
        "  q = 1/(1+np.exp(-z))\n",
        "  eps=0.00001\n",
        "  loss = -sum((y*np.log2(q+eps)+(np.ones(len(y))-y)*np.log2(np.ones(len(y))-q+eps)))\n",
        "  logistic_loss.append(loss)\n",
        "  dw = ((q-y).dot(M_train) * (1/num_samples))\n",
        "  db = sum((q-y))/num_samples\n",
        "  weights = weights - lr*dw\n",
        "  bias = bias - lr*db\n",
        "plt.plot(range(1,n_iters),logistic_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "#loss = sum(-(np.ones(len(y))*np.log2(q)+(np.ones(len(y))-y)*np.log2(np.ones(len(y))-q)))"
      ],
      "metadata": {
        "id": "zxAAVv90f5Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test dataset using the trained logistic regression model\n",
        "z = M_test.dot(weights)+bias\n",
        "q = 1/(1+np.exp(-z))\n",
        "y_test_pred=[int(ql > 0.5) for ql in q]"
      ],
      "metadata": {
        "id": "A5ntxH--f8eS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the accuracy of the performance of the model\n",
        "y_test=[int(l == \"positive\") for l in labels_test]\n",
        "acc_test=[int(yp == y_test[s]) for s,yp in enumerate(y_test_pred)]\n",
        "print(sum(acc_test)/len(acc_test))"
      ],
      "metadata": {
        "id": "qsSAjDryf_BB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_test_pred=[\"positive\" if s == 1 else \"negative\" for s in y_test_pred]\n",
        "\n",
        "true_positives=sum([int(yp == \"positive\" and labels_test[s] == \"positive\") for s,yp in enumerate(labels_test_pred)])\n",
        "false_positives=sum([int(yp == \"positive\" and labels_test[s] == \"negative\") for s,yp in enumerate(labels_test_pred)])\n",
        "false_negatives=sum([int(yp == \"negative\" and labels_test[s] == \"positive\") for s,yp in enumerate(labels_test_pred)])\n",
        "precision_test=true_positives/(true_positives+false_positives)\n",
        "recall_test=true_positives/(true_positives+false_negatives)\n",
        "f1_score_test= 2 * (precision_test * recall_test) / (precision_test + recall_test)\n",
        "print(f\"test data precision:{precision_test}\")\n",
        "print(f\"test data recall:{recall_test}\")\n",
        "print(f\"test data F1-Score:{f1_score_test}\")"
      ],
      "metadata": {
        "id": "58cepuy7gEDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = M_train.dot(weights)+bias\n",
        "q = 1/(1+np.exp(-z))\n",
        "y_train_pred=[int(ql > 0.5) for ql in q]\n",
        "labels_train_pred=[\"positive\" if s == 1 else \"negative\" for s in y_train_pred]\n",
        "\n",
        "true_positives=sum([int(yp == \"positive\" and labels_train[s] == \"positive\") for s,yp in enumerate(labels_train_pred)])\n",
        "false_positives=sum([int(yp == \"positive\" and labels_train[s] == \"negative\") for s,yp in enumerate(labels_train_pred)])\n",
        "false_negatives=sum([int(yp == \"negative\" and labels_train[s] == \"positive\") for s,yp in enumerate(labels_train_pred)])\n",
        "precision_train=true_positives/(true_positives+false_positives)\n",
        "recall_train=true_positives/(true_positives+false_negatives)\n",
        "f1_score_train= 2 * (precision_train * recall_train) / (precision_train + recall_train)\n",
        "print(f\"training data precision: {precision_train}\")\n",
        "print(f\"training data recall: {recall_train}\")\n",
        "print(f\"training data F1-Score:{f1_score_train}\")"
      ],
      "metadata": {
        "id": "_XE1oWNsgG3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile README.md\n",
        "# Sentiment Classification Project\n",
        "\n",
        "## Project Description\n",
        "This project implements logistic regression with one-hot encoding to classify the sentiment of Amazon product reviews.\n",
        "\n",
        "## How to Run\n",
        "1. Clone the repository: https://github.com/ydeng9950/LELA60331-Coursework\n",
        "2. Run the first cell to download the raw dataset.\n",
        "3. Run the second cell to load the necessary libraries.\n",
        "4. Run the third cell to organize the data for preprocessing.\n",
        "5. Run the fourth cell to tokenize the reviews, identify the 10000 most frequent words and create the vocabulary for one-hot encoding.\n",
        "6. Run the fifth cell to create the one-hot encoded matrix.\n",
        "7. Run the sixth cell to split the data into test and training set.\n",
        "8. Run the seventh cell to train the logistic regression classifier on training data.\n",
        "9. Run the eighth cell to make predictions on the test data.\n",
        "10. Run the ninth cell to calculate the accuracy of the model.\n",
        "11. Run the tenth cell to calculate the precision, recall and F1-score of the model on the test data.\n",
        "12. Run the eleventh cell to calculate the precision, recall and F1-score of the model on the training data.\n",
        "\n",
        "## References\n",
        "The dataset used in this project was provided by Dr.Colin Bannard and can be accessed here: (https://colab.research.google.com/github/cbannard/lela60331_24-25/blob/main/coursework/coursework_data_import.ipynb)."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWgdNnQEgN-T",
        "outputId": "02712927-6fc1-48eb-ec10-c69c8821e9cd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing README.md\n"
          ]
        }
      ]
    }
  ]
}